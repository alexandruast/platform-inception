<source>
	@type  forward
	port  24224
</source>

<source>
	@type syslog
	port 5140
	tag syslog
</source>

<filter docker.**>
	@type concat
	stream_identity_key container_id
	multiline_start_regexp /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3}/
	flush_interval 5
	key log
	timeout_label @NORMAL
</filter>

<match *.**>
	@type relabel
	@label @NORMAL
</match>

<label @NORMAL>
	<filter docker.**>
		@type parser
		key_name log
		<parse>
			@type multiline_grok
				<grok>
     			pattern %{TIMESTAMP_ISO8601:time}\s*\[(?<thread>[A-Za-z0-9-]+)\]\s*%{LOGLEVEL:log_level}\s*\[%{BASE16NUM:correlation_id}\]?\s*%{GREEDYDATA:message}
    		</grok>
    		<grok>
      		# Catch all
      		pattern %{GREEDYDATA:message}
    		</grok>
			multiline_start_regexp /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3}/
		</parse>
	</filter>

	<filter docker.**>
		@type record_transformer
		remove_keys container_name, source, grokfailure
		<record>
			container_id ${tag_parts[4]}
			formatted_time ${time}
			_metadata {"category": "/${tag_parts[1]}/docker/logs", "source": "${tag_parts[2]}", "host": "${tag_parts[3]}"}
		</record>
	</filter>

	<filter syslog.**>
		@type record_transformer
		enable_ruby true
		<record>
			_metadata {"category": "/syslog/${tag_parts * '/'}", "source": "syslog", "host": "${host}"}
		</record>
	</filter>

	<match *.**>
		@type copy

		# Local cache - logrotate this!
		<store>
		  @type file
		  path         /fluentd/log/data.*.log
		  symlink_path /fluentd/log/data.log
		  append       true
		  time_slice_format %Y%m%d
		  time_slice_wait   10m
		  time_format       %Y%m%dT%H%M%S%z
		</store>

		# Remote destination
		#<store>
		#	Your specific remote logging directives here
		#</store>
	</match>

</label>
